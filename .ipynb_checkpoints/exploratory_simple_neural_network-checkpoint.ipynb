{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e9c1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a9335",
   "metadata": {},
   "source": [
    "Activation Function (sigmoid):\n",
    "$$\\sigma(x) = \\frac{1} {1 + e^{-x}}$$\n",
    "\n",
    "Derivative of Activation Function:\n",
    "$$\\sigma'(x) = \\sigma'(x)(1-\\sigma'(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b06d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \n",
    "    # Simple 2 layer neural network\n",
    "    # First layer has as a node for each feature \n",
    "    # Second layer has a configurable number of nodes that map to 1 output\n",
    "    def __init__(self, num_of_features, second_layer_size):\n",
    "        self.weights0 = np.random.rand(num_of_features, second_layer_size)\n",
    "        self.weights1 = np.random.rand(second_layer_size, 1)\n",
    "    \n",
    "    def activation_func(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def activation_func_derivative(self, x):\n",
    "        return self.activation_func(x)*(1-self.activation_func(x))\n",
    "        \n",
    "    def predict(self, features):\n",
    "        results0 = self.activation_func(features.dot(self.weights0))\n",
    "        results1 = self.activation_func(results0.dot(self.weights1))\n",
    "        return results1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23ae05",
   "metadata": {},
   "source": [
    "For this simple example, we will use mean squared error for our loss function:\n",
    "$$MSE = \\sum_{i=1}^{n}(y_i-\\sigma(w_1(\\sigma(w_0x_0+b_0))+b_1))^2$$\n",
    "\n",
    "Ideally, for a logistic regression neural network classifier, the log loss would be the preferred loss function as it generates a convex curve while MSE does not. But for this simple example, we will proceed with using MSE as our loss function so the derivative for back propogation is easier to follow.\n",
    "\n",
    "$$Log Loss = \\sum_{i=1}^{n}(y_i\\log(\\sigma(w_1(\\sigma(w_0x_0+b_0))+b_1)) + (1-y_i)\\log(1-\\sigma(w_1(\\sigma(w_0x_0+b_0))+b_1)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c58114",
   "metadata": {},
   "source": [
    "Note, these formulas applicable for batch gradient descent because we are summing all data for each single step.\n",
    "\n",
    "Gradient with respect to layer 1 for backpropagation:\n",
    "$$\\nabla(\\boldsymbol{w_1}) = \\sum_{i=1}^{n}(2/n)(\\sigma(w_1x_1+b_1)-y_i)(\\sigma'(w_1x_1+b_1))(x_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4447a09",
   "metadata": {},
   "source": [
    "This formula can be mapped to the code below as follows:  \n",
    "$$\\texttt{get_errors(): }\\sigma(w_1x_1+b_1)-y_i$$\n",
    "$$\\texttt{activation_func_derivative(results1): }\\sigma'(w_1x_1+b_1)$$\n",
    "$$\\texttt{results0: }x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4a463",
   "metadata": {},
   "source": [
    "Gradient with respoect to layer 0 for backpropagation:\n",
    "$$\\nabla(\\boldsymbol{w_0}) = \\sum_{i=1}^{n}(2/n)(\\sigma(w_1(\\sigma(w_0x_0+b_0)+b_1))-y_i)\\sigma'(w_1(\\sigma(w_0x_0+b_0)+b_1))(w_1)\\sigma'(w_0x_0+b_0)(x_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1603ea",
   "metadata": {},
   "source": [
    "This formula can be mapped to the code below as follows:  \n",
    "$$\\texttt{get_errors(): }\\sigma(w_1(\\sigma(w_0x_0+b_0)+b_1))-y_i$$\n",
    "$$\\texttt{activation_func_derivative(results1): }\\sigma'(w_1(\\sigma(w_0x_0+b_0))+b_1)$$\n",
    "$$\\texttt{weights1: }w_1$$\n",
    "$$\\texttt{activation_func_derivative(results0): }\\sigma'(w_0x_0+b_0)$$\n",
    "$$\\texttt{features: }x_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a88b5",
   "metadata": {},
   "source": [
    "Notice that just for a simple two layer network, the derivative for first layer becomes large quickly through the chain rule. Most modern deep learning libraries contain an automatic differentiation engines built upon computation graphs to help with this. Autograd is an example of this from Pytorch: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c16ae5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def get_errors(self, labels, predictions):\n",
    "        return labels - predictions\n",
    "    \n",
    "    def get_mean_squared_error(self, errors):\n",
    "        return np.sum(np.square(errors))/errors.size \n",
    "        \n",
    "    def feedforward(self, nn, features):\n",
    "        results0 = nn.activation_func(features.dot(nn.weights0))\n",
    "        results1 = nn.activation_func(results0.dot(nn.weights1))\n",
    "        return results0, results1\n",
    "    \n",
    "    def backpropagate(self, nn, features, results0, results1, errors, learning_rate):\n",
    "        weights1_delta = (2/errors.size)*results0.T.dot(errors*nn.activation_func_derivative(results1))\n",
    "        weights0_delta = (2/errors.size)*features.T.dot(((errors*nn.activation_func_derivative(results1)).dot(nn.weights1.T))*(nn.activation_func_derivative(results0)))\n",
    "                \n",
    "        nn.weights1 += learning_rate*weights1_delta\n",
    "        nn.weights0 += learning_rate*weights0_delta\n",
    "        \n",
    "    def train(self, nn, features, labels, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            results0, results1 = self.feedforward(nn, features)\n",
    "            errors = self.get_errors(labels, results1)\n",
    "            mean_squared_errors = self.get_mean_squared_error(errors)\n",
    "            print(\"At epoch:\", epoch, \", MSE = \", mean_squared_errors) \n",
    "            self.backpropagate(nn, features, results0, results1, errors, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1cba17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d72205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some simple training data just to check basic funcationality\n",
    "simple_features = np.array([[-3,-3],\n",
    "                            [-3,3],\n",
    "                            [3,-3],\n",
    "                            [3,3]])\n",
    "\n",
    "simple_labels = np.array([[0], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b859320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 0 , MSE =  0.09443632415180275\n",
      "At epoch: 1 , MSE =  0.0939249639117902\n",
      "At epoch: 2 , MSE =  0.09342445176007837\n",
      "At epoch: 3 , MSE =  0.09293452201053289\n",
      "At epoch: 4 , MSE =  0.09245491571818273\n",
      "At epoch: 5 , MSE =  0.09198538055157912\n",
      "At epoch: 6 , MSE =  0.09152567066189136\n",
      "At epoch: 7 , MSE =  0.0910755465494814\n",
      "At epoch: 8 , MSE =  0.09063477492862373\n",
      "At epoch: 9 , MSE =  0.09020312859096831\n",
      "At epoch: 10 , MSE =  0.08978038626827843\n",
      "At epoch: 11 , MSE =  0.08936633249492051\n",
      "At epoch: 12 , MSE =  0.08896075747052642\n",
      "At epoch: 13 , MSE =  0.08856345692320242\n",
      "At epoch: 14 , MSE =  0.08817423197361389\n",
      "At epoch: 15 , MSE =  0.0877928890002345\n",
      "At epoch: 16 , MSE =  0.08741923950601274\n",
      "At epoch: 17 , MSE =  0.08705309998667429\n",
      "At epoch: 18 , MSE =  0.08669429180084975\n",
      "At epoch: 19 , MSE =  0.0863426410421894\n",
      "At epoch: 20 , MSE =  0.08599797841360185\n",
      "At epoch: 21 , MSE =  0.08566013910373184\n",
      "At epoch: 22 , MSE =  0.08532896266577188\n",
      "At epoch: 23 , MSE =  0.08500429289868434\n",
      "At epoch: 24 , MSE =  0.08468597773089555\n",
      "At epoch: 25 , MSE =  0.08437386910650659\n",
      "At epoch: 26 , MSE =  0.08406782287405577\n",
      "At epoch: 27 , MSE =  0.08376769867785294\n",
      "At epoch: 28 , MSE =  0.08347335985189844\n",
      "At epoch: 29 , MSE =  0.08318467331638743\n",
      "At epoch: 30 , MSE =  0.08290150947679584\n",
      "At epoch: 31 , MSE =  0.08262374212553299\n",
      "At epoch: 32 , MSE =  0.08235124834614355\n",
      "At epoch: 33 , MSE =  0.08208390842003324\n",
      "At epoch: 34 , MSE =  0.08182160573568989\n",
      "At epoch: 35 , MSE =  0.08156422670036581\n",
      "At epoch: 36 , MSE =  0.0813116606541851\n",
      "At epoch: 37 , MSE =  0.08106379978663611\n",
      "At epoch: 38 , MSE =  0.08082053905540698\n",
      "At epoch: 39 , MSE =  0.08058177610751927\n",
      "At epoch: 40 , MSE =  0.0803474112027145\n",
      "At epoch: 41 , MSE =  0.08011734713904586\n",
      "At epoch: 42 , MSE =  0.07989148918062648\n",
      "At epoch: 43 , MSE =  0.07966974498748557\n",
      "At epoch: 44 , MSE =  0.07945202454748221\n",
      "At epoch: 45 , MSE =  0.07923824011022755\n",
      "At epoch: 46 , MSE =  0.07902830612296448\n",
      "At epoch: 47 , MSE =  0.07882213916835497\n",
      "At epoch: 48 , MSE =  0.07861965790412588\n",
      "At epoch: 49 , MSE =  0.07842078300452275\n",
      "At epoch: 50 , MSE =  0.07822543710352294\n",
      "At epoch: 51 , MSE =  0.07803354473975994\n",
      "At epoch: 52 , MSE =  0.0778450323031102\n",
      "At epoch: 53 , MSE =  0.07765982798289588\n",
      "At epoch: 54 , MSE =  0.07747786171765718\n",
      "At epoch: 55 , MSE =  0.07729906514644797\n",
      "At epoch: 56 , MSE =  0.07712337156161064\n",
      "At epoch: 57 , MSE =  0.07695071586298627\n",
      "At epoch: 58 , MSE =  0.07678103451351681\n",
      "At epoch: 59 , MSE =  0.0766142654961978\n",
      "At epoch: 60 , MSE =  0.0764503482723404\n",
      "At epoch: 61 , MSE =  0.0762892237411026\n",
      "At epoch: 62 , MSE =  0.07613083420025106\n",
      "At epoch: 63 , MSE =  0.0759751233081151\n",
      "At epoch: 64 , MSE =  0.07582203604669589\n",
      "At epoch: 65 , MSE =  0.07567151868589515\n",
      "At epoch: 66 , MSE =  0.07552351874882775\n",
      "At epoch: 67 , MSE =  0.07537798497818447\n",
      "At epoch: 68 , MSE =  0.07523486730361188\n",
      "At epoch: 69 , MSE =  0.07509411681007655\n",
      "At epoch: 70 , MSE =  0.07495568570718328\n",
      "At epoch: 71 , MSE =  0.07481952729941602\n",
      "At epoch: 72 , MSE =  0.07468559595727292\n",
      "At epoch: 73 , MSE =  0.07455384708926631\n",
      "At epoch: 74 , MSE =  0.07442423711476014\n",
      "At epoch: 75 , MSE =  0.07429672343761855\n",
      "At epoch: 76 , MSE =  0.07417126442063855\n",
      "At epoch: 77 , MSE =  0.07404781936074255\n",
      "At epoch: 78 , MSE =  0.07392634846490587\n",
      "At epoch: 79 , MSE =  0.07380681282679595\n",
      "At epoch: 80 , MSE =  0.07368917440409976\n",
      "At epoch: 81 , MSE =  0.07357339599651806\n",
      "At epoch: 82 , MSE =  0.07345944122440456\n",
      "At epoch: 83 , MSE =  0.07334727450802932\n",
      "At epoch: 84 , MSE =  0.07323686104744656\n",
      "At epoch: 85 , MSE =  0.07312816680294709\n",
      "At epoch: 86 , MSE =  0.07302115847607715\n",
      "At epoch: 87 , MSE =  0.07291580349120465\n",
      "At epoch: 88 , MSE =  0.0728120699776163\n",
      "At epoch: 89 , MSE =  0.07270992675212773\n",
      "At epoch: 90 , MSE =  0.07260934330219079\n",
      "At epoch: 91 , MSE =  0.0725102897694818\n",
      "At epoch: 92 , MSE =  0.07241273693395556\n",
      "At epoch: 93 , MSE =  0.0723166561983503\n",
      "At epoch: 94 , MSE =  0.07222201957312925\n",
      "At epoch: 95 , MSE =  0.07212879966184477\n",
      "At epoch: 96 , MSE =  0.07203696964691189\n",
      "At epoch: 97 , MSE =  0.07194650327577819\n",
      "At epoch: 98 , MSE =  0.0718573748474776\n",
      "At epoch: 99 , MSE =  0.07176955919955572\n"
     ]
    }
   ],
   "source": [
    "simple_neural_network = SimpleNeuralNetwork(2, 10)\n",
    "trainer.train(simple_neural_network, simple_features, simple_labels, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2089f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values: [[0.50017882]\n",
      " [0.91351497]\n",
      " [0.94088746]\n",
      " [0.99408301]]\n",
      "Mean Squared Error: 0.06529695424846264\n"
     ]
    }
   ],
   "source": [
    "# Creating some simple training data just to check basic funcationality\n",
    "simple_test_data = np.array([[-10,-10],\n",
    "                             [-10,10],\n",
    "                             [10,-10],\n",
    "                             [10,10]])\n",
    "\n",
    "simple_test_labels = np.array([[0], [1], [1], [1]])\n",
    "predicted_values = simple_neural_network.predict(simple_test_data)\n",
    "print(\"Predicted Values:\", predicted_values)\n",
    "errors = trainer.get_errors(simple_test_labels, predicted_values)\n",
    "mean_squared_error = trainer.get_mean_squared_error(errors);\n",
    "print(\"Mean Squared Error:\", mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9cde18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>LongestShell</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>WholeWeight</th>\n",
       "      <th>ShuckedWeight</th>\n",
       "      <th>VisceraWeight</th>\n",
       "      <th>ShellWeight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  LongestShell  Diameter  Height  WholeWeight  ShuckedWeight  \\\n",
       "0    M         0.455     0.365   0.095       0.5140         0.2245   \n",
       "1    M         0.350     0.265   0.090       0.2255         0.0995   \n",
       "2    F         0.530     0.420   0.135       0.6770         0.2565   \n",
       "3    M         0.440     0.365   0.125       0.5160         0.2155   \n",
       "4    I         0.330     0.255   0.080       0.2050         0.0895   \n",
       "\n",
       "   VisceraWeight  ShellWeight  Rings  \n",
       "0         0.1010        0.150     15  \n",
       "1         0.0485        0.070      7  \n",
       "2         0.1415        0.210      9  \n",
       "3         0.1140        0.155     10  \n",
       "4         0.0395        0.055      7  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now that it seems to be working, lets test with some more complex data\n",
    "# Using data from here: http://archive.ics.uci.edu/ml/datasets/Abalone\n",
    "df = pd.read_csv(\"abalone.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4abbfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>LongestShell</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>WholeWeight</th>\n",
       "      <th>ShuckedWeight</th>\n",
       "      <th>VisceraWeight</th>\n",
       "      <th>ShellWeight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.330</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  LongestShell  Diameter  Height  WholeWeight  ShuckedWeight  \\\n",
       "0     0         0.455     0.365   0.095       0.5140         0.2245   \n",
       "1     0         0.350     0.265   0.090       0.2255         0.0995   \n",
       "2     1         0.530     0.420   0.135       0.6770         0.2565   \n",
       "3     0         0.440     0.365   0.125       0.5160         0.2155   \n",
       "6     1         0.530     0.415   0.150       0.7775         0.2370   \n",
       "\n",
       "   VisceraWeight  ShellWeight  Rings  \n",
       "0         0.1010        0.150     15  \n",
       "1         0.0485        0.070      7  \n",
       "2         0.1415        0.210      9  \n",
       "3         0.1140        0.155     10  \n",
       "6         0.1415        0.330     20  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing a bit of preprocessing\n",
    "def string_to_binary(string):\n",
    "    if string == \"F\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df = df.loc[df['Type'] != \"I\"]\n",
    "df['Type'] = df['Type'].apply(string_to_binary)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b54b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and testing data\n",
    "training_data, testing_data = train_test_split(df, test_size=0.2)\n",
    "training_features = training_data.iloc[:, ~training_data.columns.isin(['Type'])].values\n",
    "training_labels = training_data['Type'].values.reshape(len(training_features),1)\n",
    "testing_features = testing_data.iloc[:, ~testing_data.columns.isin(['Type'])].values\n",
    "testing_labels = testing_data['Type'].values.reshape(len(testing_features),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed983a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 0 , MSE =  0.529285630884817\n",
      "At epoch: 1 , MSE =  0.5270891920964527\n",
      "At epoch: 2 , MSE =  0.5242630736487769\n",
      "At epoch: 3 , MSE =  0.5206570850911936\n",
      "At epoch: 4 , MSE =  0.5161392402441239\n",
      "At epoch: 5 , MSE =  0.5106186428667963\n",
      "At epoch: 6 , MSE =  0.5040438899478759\n",
      "At epoch: 7 , MSE =  0.4963879851004707\n",
      "At epoch: 8 , MSE =  0.48763785356937184\n",
      "At epoch: 9 , MSE =  0.4777923052149453\n",
      "At epoch: 10 , MSE =  0.46686599407436113\n",
      "At epoch: 11 , MSE =  0.4548973885448366\n",
      "At epoch: 12 , MSE =  0.4419589215511727\n",
      "At epoch: 13 , MSE =  0.4281669938577057\n",
      "At epoch: 14 , MSE =  0.41368918194898585\n",
      "At epoch: 15 , MSE =  0.39874604156208115\n",
      "At epoch: 16 , MSE =  0.3836053919740012\n",
      "At epoch: 17 , MSE =  0.3685680809635634\n",
      "At epoch: 18 , MSE =  0.3539459957710742\n",
      "At epoch: 19 , MSE =  0.34003520953934213\n",
      "At epoch: 20 , MSE =  0.3270890030744115\n",
      "At epoch: 21 , MSE =  0.315296251968936\n",
      "At epoch: 22 , MSE =  0.3047696618755637\n",
      "At epoch: 23 , MSE =  0.2955456396119367\n",
      "At epoch: 24 , MSE =  0.28759429107546886\n",
      "At epoch: 25 , MSE =  0.28083566720168457\n",
      "At epoch: 26 , MSE =  0.27515781006186685\n",
      "At epoch: 27 , MSE =  0.2704331167911767\n",
      "At epoch: 28 , MSE =  0.2665311334503725\n",
      "At epoch: 29 , MSE =  0.2633273169801319\n",
      "At epoch: 30 , MSE =  0.2607082063293632\n",
      "At epoch: 31 , MSE =  0.25857383186557675\n",
      "At epoch: 32 , MSE =  0.25683823100985037\n",
      "At epoch: 33 , MSE =  0.25542880180410205\n",
      "At epoch: 34 , MSE =  0.25428503674801484\n",
      "At epoch: 35 , MSE =  0.2533570020012496\n",
      "At epoch: 36 , MSE =  0.2526037871144903\n",
      "At epoch: 37 , MSE =  0.25199205091779914\n",
      "At epoch: 38 , MSE =  0.25149472370477827\n",
      "At epoch: 39 , MSE =  0.25108988566317136\n",
      "At epoch: 40 , MSE =  0.250759818653711\n",
      "At epoch: 41 , MSE =  0.2504902167584178\n",
      "At epoch: 42 , MSE =  0.2502695361429815\n",
      "At epoch: 43 , MSE =  0.2500884638048948\n",
      "At epoch: 44 , MSE =  0.24993948587422404\n",
      "At epoch: 45 , MSE =  0.24981653818652527\n",
      "At epoch: 46 , MSE =  0.2497147242169241\n",
      "At epoch: 47 , MSE =  0.24963008780371568\n",
      "At epoch: 48 , MSE =  0.24955943022859498\n",
      "At epoch: 49 , MSE =  0.24950016309063894\n",
      "At epoch: 50 , MSE =  0.24945019000049662\n",
      "At epoch: 51 , MSE =  0.2494078114467048\n",
      "At epoch: 52 , MSE =  0.24937164827711844\n",
      "At epoch: 53 , MSE =  0.24934058012840896\n",
      "At epoch: 54 , MSE =  0.24931369585787563\n",
      "At epoch: 55 , MSE =  0.24929025361376428\n",
      "At epoch: 56 , MSE =  0.24926964864836398\n",
      "At epoch: 57 , MSE =  0.2492513873538592\n",
      "At epoch: 58 , MSE =  0.2492350663020751\n",
      "At epoch: 59 , MSE =  0.2492203553104861\n",
      "At epoch: 60 , MSE =  0.24920698375001915\n",
      "At epoch: 61 , MSE =  0.24919472946484975\n",
      "At epoch: 62 , MSE =  0.24918340979826575\n",
      "At epoch: 63 , MSE =  0.24917287431791788\n",
      "At epoch: 64 , MSE =  0.24916299891333524\n",
      "At epoch: 65 , MSE =  0.24915368100239504\n",
      "At epoch: 66 , MSE =  0.24914483563465006\n",
      "At epoch: 67 , MSE =  0.24913639232054796\n",
      "At epoch: 68 , MSE =  0.24912829244863458\n",
      "At epoch: 69 , MSE =  0.24912048717941995\n",
      "At epoch: 70 , MSE =  0.24911293572598117\n",
      "At epoch: 71 , MSE =  0.2491056039486111\n",
      "At epoch: 72 , MSE =  0.24909846320470802\n",
      "At epoch: 73 , MSE =  0.24909148940630466\n",
      "At epoch: 74 , MSE =  0.249084662246671\n",
      "At epoch: 75 , MSE =  0.24907796456472805\n",
      "At epoch: 76 , MSE =  0.24907138182190652\n",
      "At epoch: 77 , MSE =  0.24906490167085396\n",
      "At epoch: 78 , MSE =  0.2490585135992555\n",
      "At epoch: 79 , MSE =  0.24905220863515481\n",
      "At epoch: 80 , MSE =  0.24904597910269677\n",
      "At epoch: 81 , MSE =  0.24903981841926395\n",
      "At epoch: 82 , MSE =  0.2490337209266449\n",
      "At epoch: 83 , MSE =  0.24902768175022488\n",
      "At epoch: 84 , MSE =  0.24902169668128682\n",
      "At epoch: 85 , MSE =  0.2490157620784045\n",
      "At epoch: 86 , MSE =  0.24900987478463707\n",
      "At epoch: 87 , MSE =  0.2490040320578239\n",
      "At epoch: 88 , MSE =  0.24899823151176437\n",
      "At epoch: 89 , MSE =  0.24899247106645767\n",
      "At epoch: 90 , MSE =  0.24898674890590244\n",
      "At epoch: 91 , MSE =  0.2489810634422153\n",
      "At epoch: 92 , MSE =  0.2489754132850458\n",
      "At epoch: 93 , MSE =  0.2489697972154398\n",
      "At epoch: 94 , MSE =  0.24896421416344802\n",
      "At epoch: 95 , MSE =  0.24895866318889556\n",
      "At epoch: 96 , MSE =  0.24895314346482736\n",
      "At epoch: 97 , MSE =  0.24894765426322066\n",
      "At epoch: 98 , MSE =  0.248942194942629\n",
      "At epoch: 99 , MSE =  0.24893676493746944\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "classification_neural_network = SimpleNeuralNetwork(8, 10)\n",
    "trainer.train(classification_neural_network, training_features, training_labels, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76dda639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2479573789541469\n"
     ]
    }
   ],
   "source": [
    "# Checking performance on testing data\n",
    "# Note: not expecting optimal performance here as a non optimal loss function is being used\n",
    "predicted_values = classification_neural_network.predict(testing_features)\n",
    "errors = trainer.get_errors(testing_labels, predicted_values)\n",
    "print(\"Mean Squared Error:\", trainer.get_mean_squared_error(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3607da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
