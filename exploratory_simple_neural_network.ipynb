{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9c1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a9335",
   "metadata": {},
   "source": [
    "Activation Function (sigmoid):\n",
    "$$\\sigma(x) = \\frac{1} {1 + e^{-x}}$$\n",
    "\n",
    "Derivative of Activation Function:\n",
    "$$\\sigma'(x) = \\sigma'(x)(1-\\sigma'(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b06d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \n",
    "    # Simple 2 layer neural network\n",
    "    # First layer has as a node for each feature \n",
    "    # Second layer has a configurable number of nodes that map to 1 output\n",
    "    def __init__(self, num_of_features, second_layer_size):\n",
    "        self.weights0 = np.random.rand(num_of_features, second_layer_size)\n",
    "        self.weights1 = np.random.rand(second_layer_size, 1)\n",
    "    \n",
    "    def activation_func(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def activation_func_derivative(self, x):\n",
    "        return self.activation_func(x)*(1-self.activation_func(x))\n",
    "        \n",
    "    def predict(self, features):\n",
    "        results0 = self.activation_func(features.dot(self.weights0))\n",
    "        results1 = self.activation_func(results0.dot(self.weights1))\n",
    "        return results1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87062e86",
   "metadata": {},
   "source": [
    "$$\\texttt{Let }w_0 \\texttt{ represent the first layer's weights,}$$\n",
    "$$w_1 \\texttt{ represent the second layer's weights,}$$\n",
    "$$\\texttt{and }y_i \\texttt{ represent the training labels}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23ae05",
   "metadata": {},
   "source": [
    "For this simple example, we will use mean squared error for our loss function:\n",
    "$$MSE = L(\\boldsymbol{x_0}) = \\sum_{i=1}^{n}(y_i-\\sigma(\\boldsymbol{w_1}(\\sigma(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0))+b_1))^2$$\n",
    "\n",
    "Ideally, for a logistic regression neural network classifier, the log loss would be the preferred loss function as it generates a convex curve while MSE does not. But for this simple example, we will proceed with using MSE as our loss function so the derivative for back propogation is easier to follow.\n",
    "\n",
    "$$Log Loss = \\sum_{i=1}^{n}(y_i\\log(\\sigma(\\boldsymbol{w_1}(\\sigma(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0))+b_1)) + (1-y_i)\\log(1-\\sigma(\\boldsymbol{w_1}(\\sigma(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0))+b_1)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c58114",
   "metadata": {},
   "source": [
    "Note, these formulas applicable for batch gradient descent because we are summing all data for each single step.\n",
    "\n",
    "Gradient with respect to layer 1 for backpropagation:\n",
    "$$\\nabla L(\\boldsymbol{w_1}) = \\sum_{i=1}^{n}(2/n)(\\sigma(\\boldsymbol{w_1}\\boldsymbol{x_1}+b_1)-y_i)(\\sigma'(\\boldsymbol{w_1}\\boldsymbol{x_1}+b_1))(\\boldsymbol{x_1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4447a09",
   "metadata": {},
   "source": [
    "This formula can be mapped to the code below as follows:  \n",
    "$$\\texttt{get_errors(): }\\sigma(\\boldsymbol{w_1}\\boldsymbol{x_1}+b_1)-y_i$$\n",
    "$$\\texttt{activation_func_derivative(results1): }\\sigma'(\\boldsymbol{w_1}\\boldsymbol{x_1}+b_1)$$\n",
    "$$\\texttt{results0: }\\boldsymbol{x_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4a463",
   "metadata": {},
   "source": [
    "Gradient with respoect to layer 0 for backpropagation:\n",
    "$$\\nabla(L\\boldsymbol{w_0}) = \\sum_{i=1}^{n}(2/n)(\\sigma(\\boldsymbol{w_1}(\\sigma(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0)+b_1))-y_i)\\sigma'(\\boldsymbol{w_1}(\\sigma(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0)+b_1))(\\boldsymbol{w_1})\\sigma'(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0)(\\boldsymbol{x_0})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1603ea",
   "metadata": {},
   "source": [
    "This formula can be mapped to the code below as follows:  \n",
    "$$\\texttt{get_errors(): }\\sigma(\\boldsymbol{w_1}(\\sigma(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0)+b_1))-y_i$$\n",
    "$$\\texttt{activation_func_derivative(results1): }\\sigma'(\\boldsymbol{w_1}(\\sigma(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0))+b_1)$$\n",
    "$$\\texttt{weights1: }\\boldsymbol{w_1}$$\n",
    "$$\\texttt{activation_func_derivative(results0): }\\sigma'(\\boldsymbol{w_0}\\boldsymbol{x_0}+b_0)$$\n",
    "$$\\texttt{features: }\\boldsymbol{x_0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a88b5",
   "metadata": {},
   "source": [
    "Notice that for a simple two layer network, the derivative for first layer becomes large quickly through the chain rule. Most modern deep learning libraries will contain a automatic differentiation engine built upon computation graphs to help with this for more complex models. Autograd is an example from Pytorch: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16ae5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def get_errors(self, labels, predictions):\n",
    "        return labels - predictions\n",
    "    \n",
    "    def get_mean_squared_error(self, errors):\n",
    "        return np.sum(np.square(errors))/errors.size \n",
    "        \n",
    "    def feedforward(self, nn, features):\n",
    "        results0 = nn.activation_func(features.dot(nn.weights0))\n",
    "        results1 = nn.activation_func(results0.dot(nn.weights1))\n",
    "        return results0, results1\n",
    "    \n",
    "    def backpropagate(self, nn, features, results0, results1, errors, learning_rate):\n",
    "        weights1_delta = (2/errors.size)*results0.T.dot(errors*nn.activation_func_derivative(results1))\n",
    "        weights0_delta = (2/errors.size)*features.T.dot(((errors*nn.activation_func_derivative(results1)).dot(nn.weights1.T))*(nn.activation_func_derivative(results0)))\n",
    "                \n",
    "        nn.weights1 += learning_rate*weights1_delta\n",
    "        nn.weights0 += learning_rate*weights0_delta\n",
    "        \n",
    "    def train(self, nn, features, labels, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            results0, results1 = self.feedforward(nn, features)\n",
    "            errors = self.get_errors(labels, results1)\n",
    "            mean_squared_errors = self.get_mean_squared_error(errors)\n",
    "            print(\"At epoch:\", epoch, \", MSE = \", mean_squared_errors) \n",
    "            self.backpropagate(nn, features, results0, results1, errors, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1cba17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d72205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some simple training data just to check basic funcationality\n",
    "simple_features = np.array([[-3,-3],\n",
    "                            [-3,3],\n",
    "                            [3,-3],\n",
    "                            [3,3]])\n",
    "\n",
    "simple_labels = np.array([[0], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b859320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 0 , MSE =  0.09576415704131132\n",
      "At epoch: 1 , MSE =  0.09512216288054746\n",
      "At epoch: 2 , MSE =  0.09449519608429688\n",
      "At epoch: 3 , MSE =  0.09388287364821901\n",
      "At epoch: 4 , MSE =  0.0932848210078843\n",
      "At epoch: 5 , MSE =  0.09270067206268927\n",
      "At epoch: 6 , MSE =  0.09213006917363828\n",
      "At epoch: 7 , MSE =  0.09157266313785678\n",
      "At epoch: 8 , MSE =  0.09102811314247154\n",
      "At epoch: 9 , MSE =  0.09049608670027723\n",
      "At epoch: 10 , MSE =  0.08997625956940519\n",
      "At epoch: 11 , MSE =  0.08946831565902003\n",
      "At epoch: 12 , MSE =  0.08897194692289354\n",
      "At epoch: 13 , MSE =  0.08848685324253829\n",
      "At epoch: 14 , MSE =  0.08801274230143098\n",
      "At epoch: 15 , MSE =  0.08754932945171366\n",
      "At epoch: 16 , MSE =  0.08709633757462774\n",
      "At epoch: 17 , MSE =  0.08665349693581538\n",
      "At epoch: 18 , MSE =  0.0862205450365102\n",
      "At epoch: 19 , MSE =  0.08579722646153623\n",
      "At epoch: 20 , MSE =  0.08538329272493933\n",
      "At epoch: 21 , MSE =  0.08497850211398723\n",
      "At epoch: 22 , MSE =  0.08458261953219702\n",
      "At epoch: 23 , MSE =  0.0841954163419738\n",
      "At epoch: 24 , MSE =  0.08381667020737844\n",
      "At epoch: 25 , MSE =  0.08344616493748326\n",
      "At epoch: 26 , MSE =  0.08308369033071608\n",
      "At epoch: 27 , MSE =  0.08272904202054687\n",
      "At epoch: 28 , MSE =  0.08238202132282181\n",
      "At epoch: 29 , MSE =  0.08204243508501154\n",
      "At epoch: 30 , MSE =  0.08171009553760145\n",
      "At epoch: 31 , MSE =  0.08138482014781755\n",
      "At epoch: 32 , MSE =  0.0810664314758534\n",
      "At epoch: 33 , MSE =  0.0807547570337334\n",
      "At epoch: 34 , MSE =  0.08044962914692526\n",
      "At epoch: 35 , MSE =  0.08015088481879179\n",
      "At epoch: 36 , MSE =  0.07985836559795148\n",
      "At epoch: 37 , MSE =  0.07957191744860152\n",
      "At epoch: 38 , MSE =  0.07929139062383961\n",
      "At epoch: 39 , MSE =  0.07901663954200772\n",
      "At epoch: 40 , MSE =  0.07874752266606866\n",
      "At epoch: 41 , MSE =  0.07848390238601527\n",
      "At epoch: 42 , MSE =  0.07822564490430182\n",
      "At epoch: 43 , MSE =  0.07797262012428038\n",
      "At epoch: 44 , MSE =  0.07772470154161563\n",
      "At epoch: 45 , MSE =  0.07748176613864692\n",
      "At epoch: 46 , MSE =  0.07724369428165971\n",
      "At epoch: 47 , MSE =  0.07701036962102437\n",
      "At epoch: 48 , MSE =  0.07678167899415644\n",
      "At epoch: 49 , MSE =  0.07655751233124856\n",
      "At epoch: 50 , MSE =  0.07633776256372181\n",
      "At epoch: 51 , MSE =  0.07612232553534228\n",
      "At epoch: 52 , MSE =  0.07591109991594615\n",
      "At epoch: 53 , MSE =  0.07570398711771553\n",
      "At epoch: 54 , MSE =  0.07550089121394667\n",
      "At epoch: 55 , MSE =  0.07530171886024999\n",
      "At epoch: 56 , MSE =  0.07510637921812348\n",
      "At epoch: 57 , MSE =  0.07491478388083778\n",
      "At epoch: 58 , MSE =  0.07472684680157361\n",
      "At epoch: 59 , MSE =  0.07454248422375158\n",
      "At epoch: 60 , MSE =  0.07436161461349473\n",
      "At epoch: 61 , MSE =  0.07418415859416481\n",
      "At epoch: 62 , MSE =  0.07401003888291402\n",
      "At epoch: 63 , MSE =  0.07383918022919499\n",
      "At epoch: 64 , MSE =  0.07367150935517161\n",
      "At epoch: 65 , MSE =  0.07350695489797628\n",
      "At epoch: 66 , MSE =  0.07334544735375727\n",
      "At epoch: 67 , MSE =  0.07318691902346448\n",
      "At epoch: 68 , MSE =  0.0730313039603193\n",
      "At epoch: 69 , MSE =  0.07287853791891867\n",
      "At epoch: 70 , MSE =  0.07272855830592262\n",
      "At epoch: 71 , MSE =  0.07258130413227631\n",
      "At epoch: 72 , MSE =  0.07243671596691889\n",
      "At epoch: 73 , MSE =  0.07229473589193319\n",
      "At epoch: 74 , MSE =  0.07215530745908993\n",
      "At epoch: 75 , MSE =  0.07201837564774376\n",
      "At epoch: 76 , MSE =  0.07188388682403686\n",
      "At epoch: 77 , MSE =  0.07175178870136982\n",
      "At epoch: 78 , MSE =  0.07162203030209814\n",
      "At epoch: 79 , MSE =  0.07149456192041612\n",
      "At epoch: 80 , MSE =  0.0713693350863894\n",
      "At epoch: 81 , MSE =  0.07124630253109927\n",
      "At epoch: 82 , MSE =  0.07112541815286333\n",
      "At epoch: 83 , MSE =  0.07100663698449709\n",
      "At epoch: 84 , MSE =  0.07088991516158324\n",
      "At epoch: 85 , MSE =  0.07077520989171629\n",
      "At epoch: 86 , MSE =  0.07066247942469021\n",
      "At epoch: 87 , MSE =  0.07055168302359893\n",
      "At epoch: 88 , MSE =  0.07044278093682031\n",
      "At epoch: 89 , MSE =  0.07033573437085447\n",
      "At epoch: 90 , MSE =  0.07023050546398903\n",
      "At epoch: 91 , MSE =  0.07012705726076429\n",
      "At epoch: 92 , MSE =  0.07002535368721245\n",
      "At epoch: 93 , MSE =  0.06992535952684593\n",
      "At epoch: 94 , MSE =  0.06982704039737014\n",
      "At epoch: 95 , MSE =  0.06973036272809767\n",
      "At epoch: 96 , MSE =  0.06963529373804085\n",
      "At epoch: 97 , MSE =  0.06954180141466096\n",
      "At epoch: 98 , MSE =  0.0694498544932528\n",
      "At epoch: 99 , MSE =  0.06935942243694403\n"
     ]
    }
   ],
   "source": [
    "simple_neural_network = SimpleNeuralNetwork(2, 10)\n",
    "trainer.train(simple_neural_network, simple_features, simple_labels, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2089f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values: [[0.50003473]\n",
      " [0.90679407]\n",
      " [0.97406575]\n",
      " [0.99727043]]\n",
      "Mean Squared Error: 0.06485052790261346\n"
     ]
    }
   ],
   "source": [
    "# Creating some simple training data just to check basic funcationality\n",
    "simple_test_data = np.array([[-10,-10],\n",
    "                             [-10,10],\n",
    "                             [10,-10],\n",
    "                             [10,10]])\n",
    "\n",
    "simple_test_labels = np.array([[0], [1], [1], [1]])\n",
    "predicted_values = simple_neural_network.predict(simple_test_data)\n",
    "print(\"Predicted Values:\", predicted_values)\n",
    "errors = trainer.get_errors(simple_test_labels, predicted_values)\n",
    "mean_squared_error = trainer.get_mean_squared_error(errors);\n",
    "print(\"Mean Squared Error:\", mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9cde18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>LongestShell</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>WholeWeight</th>\n",
       "      <th>ShuckedWeight</th>\n",
       "      <th>VisceraWeight</th>\n",
       "      <th>ShellWeight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  LongestShell  Diameter  Height  WholeWeight  ShuckedWeight  \\\n",
       "0    M         0.455     0.365   0.095       0.5140         0.2245   \n",
       "1    M         0.350     0.265   0.090       0.2255         0.0995   \n",
       "2    F         0.530     0.420   0.135       0.6770         0.2565   \n",
       "3    M         0.440     0.365   0.125       0.5160         0.2155   \n",
       "4    I         0.330     0.255   0.080       0.2050         0.0895   \n",
       "\n",
       "   VisceraWeight  ShellWeight  Rings  \n",
       "0         0.1010        0.150     15  \n",
       "1         0.0485        0.070      7  \n",
       "2         0.1415        0.210      9  \n",
       "3         0.1140        0.155     10  \n",
       "4         0.0395        0.055      7  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now that it seems to be working, lets test with some more complex data\n",
    "# Using data from here: http://archive.ics.uci.edu/ml/datasets/Abalone\n",
    "df = pd.read_csv(\"abalone.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4abbfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>LongestShell</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>WholeWeight</th>\n",
       "      <th>ShuckedWeight</th>\n",
       "      <th>VisceraWeight</th>\n",
       "      <th>ShellWeight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.330</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  LongestShell  Diameter  Height  WholeWeight  ShuckedWeight  \\\n",
       "0     0         0.455     0.365   0.095       0.5140         0.2245   \n",
       "1     0         0.350     0.265   0.090       0.2255         0.0995   \n",
       "2     1         0.530     0.420   0.135       0.6770         0.2565   \n",
       "3     0         0.440     0.365   0.125       0.5160         0.2155   \n",
       "6     1         0.530     0.415   0.150       0.7775         0.2370   \n",
       "\n",
       "   VisceraWeight  ShellWeight  Rings  \n",
       "0         0.1010        0.150     15  \n",
       "1         0.0485        0.070      7  \n",
       "2         0.1415        0.210      9  \n",
       "3         0.1140        0.155     10  \n",
       "6         0.1415        0.330     20  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing a bit of preprocessing\n",
    "def string_to_binary(string):\n",
    "    if string == \"F\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df = df.loc[df['Type'] != \"I\"]\n",
    "df['Type'] = df['Type'].apply(string_to_binary)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b54b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and testing data\n",
    "training_data, testing_data = train_test_split(df, test_size=0.2)\n",
    "training_features = training_data.iloc[:, ~training_data.columns.isin(['Type'])].values\n",
    "training_labels = training_data['Type'].values.reshape(len(training_features),1)\n",
    "testing_features = testing_data.iloc[:, ~testing_data.columns.isin(['Type'])].values\n",
    "testing_labels = testing_data['Type'].values.reshape(len(testing_features),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed983a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 0 , MSE =  0.5222390349214945\n",
      "At epoch: 1 , MSE =  0.5175807931473139\n",
      "At epoch: 2 , MSE =  0.511954531789382\n",
      "At epoch: 3 , MSE =  0.5052089091181172\n",
      "At epoch: 4 , MSE =  0.4971947564779357\n",
      "At epoch: 5 , MSE =  0.48777935589302085\n",
      "At epoch: 6 , MSE =  0.4768657379710762\n",
      "At epoch: 7 , MSE =  0.46441595541928615\n",
      "At epoch: 8 , MSE =  0.45047541818215414\n",
      "At epoch: 9 , MSE =  0.43519324514394236\n",
      "At epoch: 10 , MSE =  0.4188320635851792\n",
      "At epoch: 11 , MSE =  0.401761004491445\n",
      "At epoch: 12 , MSE =  0.3844287502183131\n",
      "At epoch: 13 , MSE =  0.367319095742956\n",
      "At epoch: 14 , MSE =  0.35089755419011226\n",
      "At epoch: 15 , MSE =  0.3355611166245467\n",
      "At epoch: 16 , MSE =  0.3216023611904771\n",
      "At epoch: 17 , MSE =  0.30919410057957714\n",
      "At epoch: 18 , MSE =  0.2983942100274783\n",
      "At epoch: 19 , MSE =  0.28916515334455856\n",
      "At epoch: 20 , MSE =  0.2814006586236127\n",
      "At epoch: 21 , MSE =  0.27495279028739855\n",
      "At epoch: 22 , MSE =  0.2696550054280362\n",
      "At epoch: 23 , MSE =  0.26533929372507287\n",
      "At epoch: 24 , MSE =  0.2618473665745306\n",
      "At epoch: 25 , MSE =  0.25903688015292375\n",
      "At epoch: 26 , MSE =  0.2567840132980552\n",
      "At epoch: 27 , MSE =  0.25498364673750246\n",
      "At epoch: 28 , MSE =  0.2535481371268173\n",
      "At epoch: 29 , MSE =  0.2524053936207181\n",
      "At epoch: 30 , MSE =  0.251496717119074\n",
      "At epoch: 31 , MSE =  0.2507746753064501\n",
      "At epoch: 32 , MSE =  0.25020115781040414\n",
      "At epoch: 33 , MSE =  0.24974567351559493\n",
      "At epoch: 34 , MSE =  0.24938390320188814\n",
      "At epoch: 35 , MSE =  0.24909649411079063\n",
      "At epoch: 36 , MSE =  0.24886807048934564\n",
      "At epoch: 37 , MSE =  0.24868642989678486\n",
      "At epoch: 38 , MSE =  0.24854189536999455\n",
      "At epoch: 39 , MSE =  0.24842679612906696\n",
      "At epoch: 40 , MSE =  0.2483350530079795\n",
      "At epoch: 41 , MSE =  0.24826184846292193\n",
      "At epoch: 42 , MSE =  0.24820336445468089\n",
      "At epoch: 43 , MSE =  0.2481565745504276\n",
      "At epoch: 44 , MSE =  0.2481190791938095\n",
      "At epoch: 45 , MSE =  0.24808897526340795\n",
      "At epoch: 46 , MSE =  0.24806475282108353\n",
      "At epoch: 47 , MSE =  0.24804521339687932\n",
      "At epoch: 48 , MSE =  0.24802940532003442\n",
      "At epoch: 49 , MSE =  0.248016572535973\n",
      "At epoch: 50 , MSE =  0.24800611409030304\n",
      "At epoch: 51 , MSE =  0.24799755204959256\n",
      "At epoch: 52 , MSE =  0.24799050609536452\n",
      "At epoch: 53 , MSE =  0.24798467339714872\n",
      "At epoch: 54 , MSE =  0.2479798126625464\n",
      "At epoch: 55 , MSE =  0.24797573149314356\n",
      "At epoch: 56 , MSE =  0.247972276357523\n",
      "At epoch: 57 , MSE =  0.24796932463673937\n",
      "At epoch: 58 , MSE =  0.24796677831147426\n",
      "At epoch: 59 , MSE =  0.24796455895004874\n",
      "At epoch: 60 , MSE =  0.24796260372756793\n",
      "At epoch: 61 , MSE =  0.24796086226267164\n",
      "At epoch: 62 , MSE =  0.24795929410280387\n",
      "At epoch: 63 , MSE =  0.24795786672406311\n",
      "At epoch: 64 , MSE =  0.24795655393950566\n",
      "At epoch: 65 , MSE =  0.24795533463178074\n",
      "At epoch: 66 , MSE =  0.24795419174340405\n",
      "At epoch: 67 , MSE =  0.24795311147177104\n",
      "At epoch: 68 , MSE =  0.24795208262694482\n",
      "At epoch: 69 , MSE =  0.24795109611891292\n",
      "At epoch: 70 , MSE =  0.24795014454787218\n",
      "At epoch: 71 , MSE =  0.24794922187654475\n",
      "At epoch: 72 , MSE =  0.24794832316784307\n",
      "At epoch: 73 , MSE =  0.24794744437462743\n",
      "At epoch: 74 , MSE =  0.2479465821710147\n",
      "At epoch: 75 , MSE =  0.24794573381685664\n",
      "At epoch: 76 , MSE =  0.24794489704871472\n",
      "At epoch: 77 , MSE =  0.24794406999202145\n",
      "At epoch: 78 , MSE =  0.24794325109019708\n",
      "At epoch: 79 , MSE =  0.24794243904734967\n",
      "At epoch: 80 , MSE =  0.24794163278186976\n",
      "At epoch: 81 , MSE =  0.2479408313887733\n",
      "At epoch: 82 , MSE =  0.24794003410907797\n",
      "At epoch: 83 , MSE =  0.24793924030484393\n",
      "At epoch: 84 , MSE =  0.2479384494387822\n",
      "At epoch: 85 , MSE =  0.24793766105755305\n",
      "At epoch: 86 , MSE =  0.24793687477805013\n",
      "At epoch: 87 , MSE =  0.24793609027610636\n",
      "At epoch: 88 , MSE =  0.24793530727716673\n",
      "At epoch: 89 , MSE =  0.24793452554856377\n",
      "At epoch: 90 , MSE =  0.24793374489309988\n",
      "At epoch: 91 , MSE =  0.2479329651437001\n",
      "At epoch: 92 , MSE =  0.24793218615894222\n",
      "At epoch: 93 , MSE =  0.24793140781930917\n",
      "At epoch: 94 , MSE =  0.24793063002403676\n",
      "At epoch: 95 , MSE =  0.24792985268845413\n",
      "At epoch: 96 , MSE =  0.24792907574173276\n",
      "At epoch: 97 , MSE =  0.247928299124976\n",
      "At epoch: 98 , MSE =  0.24792752278959276\n",
      "At epoch: 99 , MSE =  0.2479267466959083\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "classification_neural_network = SimpleNeuralNetwork(8, 10)\n",
    "trainer.train(classification_neural_network, training_features, training_labels, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76dda639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.249204095367048\n"
     ]
    }
   ],
   "source": [
    "# Checking performance on testing data\n",
    "# Note: not expecting optimal performance here as a non optimal loss function is being used\n",
    "predicted_values = classification_neural_network.predict(testing_features)\n",
    "errors = trainer.get_errors(testing_labels, predicted_values)\n",
    "print(\"Mean Squared Error:\", trainer.get_mean_squared_error(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3607da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
